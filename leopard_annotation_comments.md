While annotating images of skin lesions, we encountered several obstacles and made some observations. The primary challenge in annotating these images came from the diverse forms and shapes of the skin lesions. Below, we further explain the complications we encountered during the process of annotaiton of the skin lesions. 

Our initial challenge was grappling with the variability in appearance. A more effective approach might have been to analyze each lesion individually before annotating, which could have provided us with some insight adn awareness of the different types, which could possibly avoided some issues. This step could have led us to do a group discussions on how to proceed with the images, given their significant variability. Setting boundaries was particularly challenging for lesions that were barely visible or whose borders blended in with the patient's skin, and for those lesions that were small and widely dispersed. 

Deciding on the scope of inclusion was critical to the annotation process. including too much could cause noise, complicating the algorithm's learning process, while including too little, we could potentially lose essential details necessary for accurate diagnoses. The lack of strict or precise guidelines and the delegation of image annotation to five different students resulted in inconsistent data, which could potentially worsened the quality of input for our algorithm.

Lastly, the software we used was introduced through a hastily conducted tutorial, and its first-time application to this task likely contributed to the imprecision and inconsistency of the annotations. The use of a mouse or touchpad, instead of a pen, for annotation also made it more inaccuracies. The level of detail and precision in marking varied significantly from one anotator, affects the overall quality of our data. This led us to deliberate on the extent of the area to be marked and consider the implications for our algorithm.

In conclusion, even after we've really tried our best to accurately mark up these images, it turns out the annotations can still be pretty inconsistent and off the mark. This basically means the data we feed into our algorithm isn't as good as it could be, which might lead to getting the diagnosis wrong.
