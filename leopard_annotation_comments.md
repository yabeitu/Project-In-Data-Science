While annotating images of skin lesions, we faced many difficulties and made some observations. The primary challenge in annotating these images, were to know how to handle the different forms and shapes of the skin lesions. Below, we further explain the complications we encountered during the process of annotaiton of the skin lesions. 

Our groups first issue was undertanding the variability in appearance of the lesions. A more better approach to start of the annotation might have been to analyze or have a look at each lesion individually before annotating, which could have provided us with some insight adn awareness of the different types. This step could have led us to do a group discussions on how to proceed with the images and some sort of consistent annotation. Setting boundaries for the inclusion/exclusion of the marking of the lesion was our biggest challenge. Some lesions were barely visible because borders of the lesions would blend in with the patient's skin, and there were lesions that were small and widely dispersed. Some of it may be due to the quality of the photos, given it's all images taken by the patients themselves using smartphones, where the quality of the lens varies and the patient's capability. 

Prior to annotating the images, we did not have a group session on deciding, how much should be included or considered, how that would affect our data and algortihm. Including too much could cause noise, complicating the algorithm's learning process. While including too little, we could potentially lose important details necessary for accurate diagnoses. The lack of strict, more precise guidelines, and the delegation of image annotation to five different students may result in inconsistent data, which could potentially worsened the quality of our data. 

Lastly, the software LabelStudio, that we used was introduced through a quick tutorial, and since it's the first time using the software without any clear guidlines to this task, it has likely contributed to the imprecision and inconsistency of the annotations. The use of a mouse or touchpad, instead of a pen, for annotation also made it more inaccurate. Another factor for precision is, the level of detail and precision in marking varies from one student to another and affects the overall quality of our data. Some images had dark spots within the marking, which shows incomplete and imprecise marking. 

In conclusion, even after we've tried our best to accurately mark up these images, it turns out the annotations can still be pretty inconsistent. This basically means the data we feed into our algorithm isn't as good as it could be, which might lead to getting the diagnosis wrong.

annotation comments L3
1. Some images could hardly be annotated, unclear and non-specific.
2. Some marks were subtle and hardly noticeable.
3. Some images were of such low quality that it led to difficulty in annotating them.
4. Missing information in metadata that can affect results: in some cases, we do not know the background-father or background-mother or if the patient smokes or drinks. 
5. In some cases we do not even know if the patient is a male or female.
6. small details can not annotates

